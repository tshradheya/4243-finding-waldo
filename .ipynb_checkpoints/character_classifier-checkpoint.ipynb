{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cyvlfeat as vlfeat\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import os.path as osp\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage.feature import hog\n",
    "\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from utils import *\n",
    "from classifier_utils import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute the above code, please make sure to run the `extract_templates.py` script before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the required params\n",
    "\n",
    "# ==========================\n",
    "feature_descriptor = \"sift\"\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "train_image_paths = []\n",
    "\n",
    "\n",
    "with open('datasets/ImageSets/val.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        test_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()\n",
    "\n",
    "with open('datasets/ImageSets/train.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        train_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All function declaration above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waldo_train_paths = []\n",
    "wenda_train_paths = []\n",
    "wizard_train_paths = []\n",
    "\n",
    "\n",
    "waldo_test_paths = []\n",
    "wenda_test_paths = []\n",
    "wizard_test_paths = []\n",
    "\n",
    "training_paths = []\n",
    "\n",
    "template_dirs = [\"updated/waldo\",\"updated/wenda\",\"updated/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs)):\n",
    "    for img_id in os.listdir(template_dirs[i]):\n",
    "        path_to_dir = os.path.join(template_dirs[i]).rstrip()\n",
    "        training_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "        if i==0:\n",
    "            waldo_train_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "        if i==1:\n",
    "            wenda_train_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "        if i==2:\n",
    "            wizard_train_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "                \n",
    "template_dirs_test = [\"updated_test/waldo\",\"updated_test/wenda\",\"updated_test/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs_test)):\n",
    "    for img_id in os.listdir(template_dirs_test[i]):\n",
    "        path_to_dir = os.path.join(template_dirs[i]).rstrip()\n",
    "        if i==0:\n",
    "            waldo_test_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "        if i==1:\n",
    "            wenda_test_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n",
    "        if i==2:\n",
    "            wizard_test_paths.append(os.path.join(path_to_dir, '{}'.format(img_id)).rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab\n",
    "print('Using the BAG-OF-SIFT representation for images')\n",
    "\n",
    "vocab_filename = 'vocab.pkl'\n",
    "\n",
    "vocab_size = 200  # Larger values will work better (to a point) but be slower to compute\n",
    "vocab = build_vocabulary(training_paths, vocab_size)\n",
    "\n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    print('{:s} saved'.format(vocab_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waldo_train_feats = bags_of_sifts(waldo_train_paths, vocab_filename)\n",
    "wenda_train_feats = bags_of_sifts(wenda_train_paths, vocab_filename)\n",
    "wizard_train_feats = bags_of_sifts(wizard_train_paths, vocab_filename)\n",
    "\n",
    "training_feats = []\n",
    "training_feats.extend(waldo_train_feats)\n",
    "training_feats.extend(wenda_train_feats)\n",
    "training_feats.extend(wizard_train_feats)\n",
    "\n",
    "\n",
    "train_labels = []\n",
    "train_labels.extend([\"waldo\"]*len(waldo_train_feats))\n",
    "train_labels.extend([\"wenda\"]*len(wenda_train_feats))\n",
    "train_labels.extend([\"wizard\"]*len(wizard_train_feats))\n",
    "\n",
    "\n",
    "print(\"done train\")\n",
    "\n",
    "\n",
    "waldo_test_feats = bags_of_sifts(waldo_test_paths, vocab_filename)\n",
    "wenda_test_feats = bags_of_sifts(wenda_test_paths, vocab_filename)\n",
    "wizard_test_feats = bags_of_sifts(wizard_test_paths, vocab_filename)\n",
    "\n",
    "test_feats = []\n",
    "test_feats.extend(waldo_test_feats)\n",
    "test_feats.extend(wenda_test_feats)\n",
    "test_feats.extend(wizard_test_feats)\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "test_labels.extend([\"waldo\"]*len(waldo_test_feats))\n",
    "test_labels.extend([\"wenda\"]*len(wenda_test_feats))\n",
    "test_labels.extend([\"wizard\"]*len(wizard_test_feats))\n",
    "\n",
    "\n",
    "print(\"done test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = svm_classify(training_feats, train_labels, test_feats)\n",
    "\n",
    "print(test_accuracy(test_labels, predicted_labels))\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following method is using only SVM with SIFT and ORB descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_characters(vocab_filename, training_feats, train_labels, test_feats)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below method is to use Haar cascade for descriptors and then using SVM to predict in those "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Waldo\n",
    "find_characters_second_check_svm(vocab_filename, training_feats, train_labels, test_feats, 0)\n",
    "\n",
    "## Wenda\n",
    "find_characters_second_check_svm(vocab_filename, training_feats, train_labels, test_feats, 1)\n",
    "\n",
    "## Wizard\n",
    "find_characters_second_check_svm(vocab_filename, training_feats, train_labels, test_feats, 2)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
