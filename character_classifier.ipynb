{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cyvlfeat as vlfeat\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import os.path as osp\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2single(im):\n",
    "    im = im.astype(np.float32) / 255\n",
    "    return im\n",
    "\n",
    "def single2im(im):\n",
    "    im *= 255\n",
    "    im = im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "def load_image(path):\n",
    "    return im2single(cv2.imread(path))[:, :, ::-1]\n",
    "\n",
    "def load_image_gray(path):\n",
    "    img = load_image(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def bags_of_sifts_spm(image_paths, vocab_filename, depth=3):\n",
    "    \"\"\"\n",
    "    Bags of sifts with spatial pyramid matching.\n",
    "\n",
    "    :param image_paths: paths to N images\n",
    "    :param vocab_filename: Path to the precomputed vocabulary.\n",
    "          This function assumes that vocab_filename exists and contains an\n",
    "          vocab_size x 128 ndarray 'vocab' where each row is a kmeans centroid\n",
    "          or visual word. This ndarray is saved to disk rather than passed in\n",
    "          as a parameter to avoid recomputing the vocabulary every run.\n",
    "    :param depth: Depth L of spatial pyramid. Divide images and compute (sum)\n",
    "          bags-of-sifts for all image partitions for all pyramid levels.\n",
    "          Refer to the explanation in the notebook, tutorial slide and the \n",
    "          original paper (Lazebnik et al. 2006.) for more details.\n",
    "\n",
    "    :return image_feats: N x d matrix, where d is the dimensionality of the\n",
    "          feature representation. In this case, d will equal the number of\n",
    "          clusters (vocab_size) times the number of regions in all pyramid levels,\n",
    "          which is 21 (1+4+16) in this specific case.\n",
    "    \"\"\"\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    vocab_size = vocab.shape[0]\n",
    "    feats = []\n",
    "    weights = [0.25, 0.25, 0.5]\n",
    "\n",
    "\n",
    "    for path in image_paths:\n",
    "        img = load_image_gray(path)\n",
    "        \n",
    "        H = img.shape[0]\n",
    "        W = img.shape[1]\n",
    "        histogram = []\n",
    "        \n",
    "        _, descriptors = vlfeat.sift.dsift(img, step=5, fast=True)\n",
    "        dist = cdist(vocab, descriptors, 'euclidean')\n",
    "        \n",
    "        \n",
    "        # Level Zero\n",
    "        min_dist_idx = np.argmin(dist, axis = 0)\n",
    "        hist = np.histogram(min_dist_idx, np.arange(201))[0] * weights[0]\n",
    "\n",
    "        if np.linalg.norm(histogram) != 0:\n",
    "            hist = hist / np.linalg.norm(hist)\n",
    "        \n",
    "        histogram.extend(hist)\n",
    "        \n",
    "        \n",
    "        # Level 1\n",
    "        blocks = split(img)\n",
    "        \n",
    "        for block in blocks:\n",
    "            _, descriptors = vlfeat.sift.dsift(block, step=5, fast=True)\n",
    "            dist = cdist(vocab, descriptors, 'euclidean')\n",
    "            min_dist_idx = np.argmin(dist, axis = 0)\n",
    "            hist, _ = np.histogram(min_dist_idx, np.arange(201))\n",
    "            hist = hist * weights[1]\n",
    "\n",
    "            if np.linalg.norm(histogram) != 0:\n",
    "                hist = hist / np.linalg.norm(hist)\n",
    "\n",
    "            histogram.extend(hist)\n",
    "            \n",
    "        \n",
    "        # Level 2\n",
    "        \n",
    "        for block in blocks:\n",
    "            sub_blocks = split(block)\n",
    "        \n",
    "            for sub_block in sub_blocks:\n",
    "                _, descriptors = vlfeat.sift.dsift(sub_block, step=5, fast=True)\n",
    "                dist = cdist(vocab, descriptors, 'euclidean')\n",
    "                min_dist_idx = np.argmin(dist, axis = 0)\n",
    "                hist, _ = np.histogram(min_dist_idx, np.arange(201))\n",
    "                hist = hist * weights[2]\n",
    "\n",
    "                if np.linalg.norm(histogram) != 0:\n",
    "                    hist = hist / np.linalg.norm(hist)\n",
    "                \n",
    "                histogram.extend(hist)\n",
    "\n",
    "        \n",
    "        feats.append(histogram)\n",
    "\n",
    "    return np.array(feats)\n",
    "\n",
    "\n",
    "def split(arr):\n",
    "    \"\"\"Split a matrix into sub-matrices.\"\"\"\n",
    "\n",
    "    half_split = np.array_split(arr, 2)\n",
    "\n",
    "    result = map(lambda x: np.array_split(x, 2, axis=1), half_split)\n",
    "    result = reduce(add, result)\n",
    "\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bags_of_sifts(image_paths, vocab_filename):\n",
    "\n",
    "    # load vocabulary\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # dummy features variable\n",
    "    feats = np.zeros((len(image_paths),len(vocab)))\n",
    "\n",
    "\n",
    "    for i, path in enumerate(image_paths):\n",
    "        \n",
    "        image = load_image_gray(path)\n",
    "        frames, descriptors = vlfeat.sift.dsift(image, step=5, fast=True)\n",
    "        \n",
    "        \n",
    "        dist = cdist(vocab, descriptors, 'euclidean')\n",
    "        min_dist_idx = np.argmin(dist, axis = 0)\n",
    "        histogram, _ = np.histogram(min_dist_idx, range(len(vocab)+1))\n",
    "        \n",
    "        if np.linalg.norm(histogram) == 0:\n",
    "            feats[i, :] = histogram\n",
    "        else:\n",
    "            feats[i, :] = histogram / np.linalg.norm(histogram)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def bags_of_sifts_image(image, vocab_filename):\n",
    "\n",
    "    # load vocabulary\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # dummy features variable\n",
    "    feats = np.zeros((1,len(vocab)))\n",
    "\n",
    "\n",
    "        \n",
    "    frames, descriptors = vlfeat.sift.dsift(image, step=5, fast=True)\n",
    "\n",
    "\n",
    "    dist = cdist(vocab, descriptors, 'euclidean')\n",
    "    min_dist_idx = np.argmin(dist, axis = 0)\n",
    "    histogram, _ = np.histogram(min_dist_idx, range(len(vocab)+1))\n",
    "\n",
    "    if np.linalg.norm(histogram) == 0:\n",
    "        feats[0, :] = histogram\n",
    "    else:\n",
    "        feats[0, :] = histogram / np.linalg.norm(histogram)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(image_paths, vocab_size):\n",
    "\n",
    "    dim = 128      # length of the SIFT descriptors that you are going to compute.\n",
    "    vocab = np.zeros((vocab_size,dim))\n",
    "    total_SIFT_features = np.zeros((20*len(image_paths), dim))\n",
    "    \n",
    "    step_size = 5;\n",
    "    features = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        img = load_image_gray(path)\n",
    "        _, descriptors = vlfeat.sift.dsift(img, step=step_size, fast=True)\n",
    "        \n",
    "        descriptors = descriptors[np.random.choice(descriptors.shape[0], 20)]\n",
    "        features.append(descriptors)\n",
    "    \n",
    "    features = np.concatenate(features, axis=0).astype('float64')\n",
    "\n",
    "    vocab = vlfeat.kmeans.kmeans(features, vocab_size) \n",
    "    \n",
    "        \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):    \n",
    "    clf = LinearSVC(C=2)\n",
    "    clf.fit(train_image_feats, train_labels)\n",
    "    test_labels = clf.predict(test_image_feats)\n",
    "\n",
    "    return test_labels\n",
    "\n",
    "\n",
    "def test_accuracy(test_labels, predicted_labels):\n",
    "    num_correct = 0\n",
    "    for i,label in enumerate(test_labels):\n",
    "        if (predicted_labels[i] == label):\n",
    "            num_correct += 1\n",
    "    return num_correct/len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All function declaration above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "124\n",
      "36\n",
      "24\n",
      "---\n",
      "13\n",
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "waldo_train_paths = []\n",
    "wenda_train_paths = []\n",
    "wizard_train_paths = []\n",
    "\n",
    "\n",
    "waldo_test_paths = []\n",
    "wenda_test_paths = []\n",
    "wizard_test_paths = []\n",
    "\n",
    "\n",
    "all_paths = []\n",
    "test_image_paths = []\n",
    "train_image_paths = []\n",
    "\n",
    "\n",
    "with open('datasets/ImageSets/val.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        test_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()\n",
    "\n",
    "with open('datasets/ImageSets/train.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        train_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "template_dirs = [\"templates/waldo\",\"templates/wenda\",\"templates/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs)):\n",
    "    for img_id in os.listdir(template_dirs[i]):\n",
    "        path_to_dir = os.path.join(template_dirs[i], '{}'.format(img_id)).rstrip()\n",
    "        if not os.path.isdir(path_to_dir):\n",
    "            continue\n",
    "        list_of_files = os.listdir(path_to_dir)\n",
    "        for file_name in list_of_files:\n",
    "            all_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==0:\n",
    "                waldo_train_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==1:\n",
    "                wenda_train_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==2:\n",
    "                wizard_train_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "                \n",
    "template_dirs_test = [\"test/templates/waldo\",\"test/templates/wenda\",\"test/templates/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs_test)):\n",
    "    for img_id in os.listdir(template_dirs_test[i]):\n",
    "        path_to_dir = os.path.join(template_dirs_test[i], '{}'.format(img_id)).rstrip()\n",
    "        if not os.path.isdir(path_to_dir):\n",
    "            continue\n",
    "        list_of_files = os.listdir(path_to_dir)\n",
    "        for file_name in list_of_files:\n",
    "#             all_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==0:\n",
    "                waldo_test_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==1:\n",
    "                wenda_test_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==2:\n",
    "                wizard_test_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "\n",
    "print(len(all_paths))\n",
    "print(len(waldo_train_paths))\n",
    "print(len(wenda_train_paths))\n",
    "print(len(wizard_train_paths))\n",
    "print(\"---\")\n",
    "print(len(waldo_test_paths))\n",
    "print(len(wenda_test_paths))\n",
    "print(len(wizard_test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BAG-OF-SIFT representation for images\n",
      "False\n",
      "vocab.pkl saved\n"
     ]
    }
   ],
   "source": [
    "# get vocab\n",
    "print('Using the BAG-OF-SIFT representation for images')\n",
    "\n",
    "vocab_filename = 'vocab.pkl'\n",
    "\n",
    "# print('No existing visual word vocabulary found. Computing one from training images')\n",
    "vocab_size = 200  # Larger values will work better (to a point) but be slower to compute\n",
    "vocab = build_vocabulary(all_paths, vocab_size)\n",
    "print(np.isnan(vocab).any())\n",
    "\n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "    print('{:s} saved'.format(vocab_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done train\n",
      "done test\n"
     ]
    }
   ],
   "source": [
    "waldo_train_feats = bags_of_sifts(waldo_train_paths, vocab_filename)\n",
    "wenda_train_feats = bags_of_sifts(wenda_train_paths, vocab_filename)\n",
    "wizard_train_feats = bags_of_sifts(wizard_train_paths, vocab_filename)\n",
    "\n",
    "training_feats = []\n",
    "training_feats.extend(waldo_train_feats)\n",
    "training_feats.extend(wenda_train_feats)\n",
    "training_feats.extend(wizard_train_feats)\n",
    "\n",
    "\n",
    "train_labels = []\n",
    "train_labels.extend([\"waldo\"]*len(waldo_train_feats))\n",
    "train_labels.extend([\"wenda\"]*len(wenda_train_feats))\n",
    "train_labels.extend([\"wizard\"]*len(wizard_train_feats))\n",
    "\n",
    "\n",
    "print(\"done train\")\n",
    "\n",
    "\n",
    "waldo_test_feats = bags_of_sifts(waldo_test_paths, vocab_filename)\n",
    "wenda_test_feats = bags_of_sifts(wenda_test_paths, vocab_filename)\n",
    "wizard_test_feats = bags_of_sifts(wizard_test_paths, vocab_filename)\n",
    "\n",
    "test_feats = []\n",
    "test_feats.extend(waldo_test_feats)\n",
    "test_feats.extend(wenda_test_feats)\n",
    "test_feats.extend(wizard_test_feats)\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "test_labels.extend([\"waldo\"]*len(waldo_test_feats))\n",
    "test_labels.extend([\"wenda\"]*len(wenda_test_feats))\n",
    "test_labels.extend([\"wizard\"]*len(wizard_test_feats))\n",
    "\n",
    "\n",
    "print(\"done test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n",
      "['waldo' 'waldo' 'waldo' 'waldo' 'waldo' 'waldo' 'wizard' 'waldo' 'wenda'\n",
      " 'waldo' 'waldo' 'waldo' 'waldo' 'wenda' 'wenda' 'waldo' 'waldo' 'wenda'\n",
      " 'waldo' 'wenda' 'waldo' 'waldo' 'wizard']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = svm_classify(training_feats, train_labels, test_feats)\n",
    "\n",
    "print(test_accuracy(test_labels, predicted_labels))\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_probability(train_image_feats, train_labels, test_image_feats):\n",
    "    \n",
    "    svc = SVC(C=2, gamma='scale',probability=True)\n",
    "    svc.fit(train_image_feats, train_labels)\n",
    "    test_probabilities = svc.predict_proba(test_image_feats)\n",
    "\n",
    "#     clf = LinearSVC(C=2, probability=True)\n",
    "#     clf.fit(train_image_feats, train_labels)\n",
    "#     test_labels = clf.predict(test_image_feats)\n",
    "\n",
    "    return test_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_characters():\n",
    "    \n",
    "    window = 64\n",
    "    f = open('datasets/ImageSets/val.txt')\n",
    "    wa = open('baseline_test/waldo.txt', 'w+')\n",
    "    we = open('baseline_test/wenda.txt', 'w+')\n",
    "    wi = open('baseline_test/wizard.txt', 'w+')\n",
    "    \n",
    "    image_id = f.readline().rstrip()\n",
    "    while image_id:\n",
    "#         image_id = \"003\"\n",
    "        image = np.asarray(plt.imread('datasets/JPEGImages/' + image_id + '.jpg'))\n",
    "        H, W, chan = image.shape\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        test_feats = []\n",
    "\n",
    "#         orb = cv2.ORB_create()\n",
    "        orb = cv2.ORB_create(nfeatures=100000, scoreType=cv2.ORB_FAST_SCORE)\n",
    "        kp, des = orb.detectAndCompute(img_gray, None)\n",
    "\n",
    "# #         minHessian = 400\n",
    "# #         detector = cv2.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "# #         kp = detector.detect(img_gray)\n",
    "\n",
    "#             fast = cv2.FastFeatureDetector_create()\n",
    "#         # find and draw the keypoints\n",
    "#         kp = fast.detect(img_gray,None)\n",
    "        img_kp = cv2.drawKeypoints(img_gray, kp, None, color=(0,0,255), flags=cv2.DrawMatchesFlags_DEFAULT)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.imshow(img_kp)\n",
    "#         plt.show()\n",
    "        \n",
    "        for idx in range(len(kp)):\n",
    "            j,i = kp[idx].pt\n",
    "\n",
    "            i = int(np.round(i))\n",
    "            j = int(np.round(j))\n",
    "            i_end = i+window\n",
    "            j_end = j+window\n",
    "            \n",
    "            i_end = min(i_end, H-1)\n",
    "            j_end = min(j_end, W-1)\n",
    "\n",
    "            img = img_gray[i:i_end,j:j_end]\n",
    "            feats = bags_of_sifts_image(img_gray, vocab_filename)\n",
    "            test_feats.extend(feats)\n",
    "\n",
    "            \n",
    "        numOfMax = 5\n",
    "        probability = svm_probability(training_feats, train_labels, test_feats)\n",
    "\n",
    "        locations = np.argpartition(-probability, numOfMax, axis =0)[:numOfMax]\n",
    "#         print(locations)\n",
    "\n",
    "        \n",
    "        for k in range(len(locations[0])):\n",
    "            for l in range(numOfMax):\n",
    "\n",
    "                y, x  = kp[locations[l][k]].pt\n",
    "\n",
    "                x = int(np.round(x))\n",
    "                y = int(np.round(y))\n",
    "                y_end = y+window\n",
    "                x_end = x+window\n",
    "\n",
    "                x_end = min(x_end, H-1)\n",
    "                y_end = min(y_end, W-1)\n",
    "\n",
    "#                 patch = img_gray[x:x_end, y:y_end]\n",
    "#                 plt.imshow(patch)\n",
    "#                 plt.show()\n",
    "\n",
    "                if (probability[locations[l][k]][k] > 0.5):\n",
    "                    if k == 0:\n",
    "                        res = image_id + ' ' + str(probability[locations[l][k]][k]) + ' ' + str(x) + ' ' + str(x_end) + ' ' + str(y) + ' ' + str(y_end) + '\\n'\n",
    "#                         print(probability[locations[l][k]][k])\n",
    "                        wa.write(res)\n",
    "                    if k == 1:\n",
    "                        res = image_id + ' ' + str(np.max(probability[locations[l][k]][k])) + ' ' + str(x) + ' ' + str(x_end) + ' ' + str(y) + ' ' + str(y_end) + '\\n'\n",
    "#                         print(res)\n",
    "                        we.write(res)\n",
    "                    if k == 2:\n",
    "                        res = image_id + ' ' + str(np.max(probability[locations[l][k]][k])) + ' ' + str(x) + ' ' + str(x_end) + ' ' + str(y) + ' ' + str(y_end) + '\\n'\n",
    "#                         print(res)\n",
    "                        wi.write(res)\n",
    "#         break\n",
    "        image_id = f.readline().rstrip()\n",
    "\n",
    "\n",
    "\n",
    "find_characters()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
